{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 量化感知训练和训练后量化的对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本文件主要是以 MNIST数据集 为例 对比 量化感知训练 和 训练后量化 两种量化方式\n",
    "# 主要分为四部分\n",
    "# 1 训练一个浮点数模型作为基础标准\n",
    "# 2 进行量化感知训练\n",
    "# 3 进行训练后量化\n",
    "# 4 比较两者结果\n",
    "# 作者：wzx\n",
    "# 修改时间：2020.12.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练浮点数模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path setting\n",
    "# save TFLite Model\n",
    "if not os.path.exists('./model_saved'):\n",
    "    os.mkdir('./model_saved')\n",
    "basic_path = './model_saved'\n",
    "model_path = os.path.join(basic_path, 'MNIST_model.h5')\n",
    "QAT_INT8_model_path = os.path.join(basic_path, 'QAT_INT8_MNIST.tflite')\n",
    "PTQ_INT8_model_path = os.path.join(basic_path, 'PTQ_INT8_MNIST.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic setting\n",
    "NUM_classes = 10\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "lr = 0.1\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 26, 26, 12)        120       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 12)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2028)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                20290     \n",
      "=================================================================\n",
      "Total params: 20,410\n",
      "Trainable params: 20,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture.\n",
    "model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "    keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "    keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# print model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "422/422 [==============================] - 4s 9ms/step - loss: 0.4552 - accuracy: 0.8762 - val_loss: 0.1849 - val_accuracy: 0.9462\n",
      "Epoch 2/5\n",
      "422/422 [==============================] - 4s 9ms/step - loss: 0.1805 - accuracy: 0.9478 - val_loss: 0.1226 - val_accuracy: 0.9655\n",
      "Epoch 3/5\n",
      "422/422 [==============================] - 4s 9ms/step - loss: 0.1317 - accuracy: 0.9626 - val_loss: 0.1004 - val_accuracy: 0.9745\n",
      "Epoch 4/5\n",
      "422/422 [==============================] - 4s 9ms/step - loss: 0.1077 - accuracy: 0.9690 - val_loss: 0.0937 - val_accuracy: 0.9765\n",
      "Epoch 5/5\n",
      "422/422 [==============================] - 4s 9ms/step - loss: 0.0936 - accuracy: 0.9732 - val_loss: 0.0855 - val_accuracy: 0.9783\n"
     ]
    }
   ],
   "source": [
    "# Train the digit classification model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=lr,\n",
    ")\n",
    "\n",
    "model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行量化感知训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer (QuantizeLaye (None, 28, 28)            3         \n",
      "_________________________________________________________________\n",
      "quant_reshape (QuantizeWrapp (None, 28, 28, 1)         1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d (QuantizeWrappe (None, 26, 26, 12)        147       \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d (Quantiz (None, 13, 13, 12)        1         \n",
      "_________________________________________________________________\n",
      "quant_flatten (QuantizeWrapp (None, 2028)              1         \n",
      "_________________________________________________________________\n",
      "quant_dense (QuantizeWrapper (None, 10)                20295     \n",
      "=================================================================\n",
      "Total params: 20,448\n",
      "Trainable params: 20,410\n",
      "Non-trainable params: 38\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 插入伪量化节点\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "# q_aware stands for for quantization aware.\n",
    "QAT_model = quantize_model(model)\n",
    "QAT_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 77ms/step - loss: 0.0949 - accuracy: 0.9744 - val_loss: 0.1161 - val_accuracy: 0.9600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9a8063fa58>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在训练集子集上重新训练，进行微调\n",
    "train_images_subset = train_images[0:1000]  # out of 60000\n",
    "train_labels_subset = train_labels[0:1000]\n",
    "\n",
    "# quantize_model requires a recompile.\n",
    "QAT_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "QAT_model.fit(train_images_subset,\n",
    "              train_labels_subset,\n",
    "              batch_size=500,\n",
    "              epochs=1,\n",
    "              validation_split=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正常训练的准确率\n",
      "Baseline test accuracy: 0.9732999801635742\n",
      "量化感知训练(未量化)的准确率\n",
      "Quant test accuracy: 0.9746000170707703\n"
     ]
    }
   ],
   "source": [
    "# 评估加入量化感知的结果\n",
    "_, baseline_model_accuracy = model.evaluate(test_images, test_labels, verbose=0)\n",
    "_, QAT_model_accuracy = QAT_model.evaluate(test_images, test_labels, verbose=0)\n",
    "\n",
    "print('正常训练的准确率')\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "print('量化感知训练(未量化)的准确率')\n",
    "print('Quant test accuracy:', QAT_model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理\n",
    "if tf.keras.backend.image_data_format() == 'channels_first':\n",
    "    x_train = train_images.reshape(train_images.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = test_images.reshape(test_images.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = train_images.reshape(train_images.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = test_images.reshape(test_images.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "batch_input_shape = (1, ) + input_shape\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# x_train /= 255\n",
    "# x_test /= 255\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了测定量化阈值，导入一段数据\n",
    "# Use the first 300 images in the post-training quantization.\n",
    "def representative_data_gen():\n",
    "    for i in range(300):\n",
    "        image = x_train[i].reshape(batch_input_shape)\n",
    "        yield [image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert TFLite model.\n",
      "WARNING:tensorflow:From /home/wzx/anaconda3/envs/TFQAT/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:465: set_learning_phase (from tensorflow.python.keras.backend) is deprecated and will be removed after 2020-10-11.\n",
      "Instructions for updating:\n",
      "Simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "WARNING:tensorflow:From /home/wzx/anaconda3/envs/TFQAT/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:105: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpi00249dc/assets\n"
     ]
    }
   ],
   "source": [
    "# 利用TF Lite后端 创建实际量化模型\n",
    "# After this, you have an actually quantized model with int8 weights and uint8 activations.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(QAT_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter._experimental_new_quantizer = True  # pylint: disable=protected-access\n",
    "\n",
    "# to enable post-training quantization with the representative dataset\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to int8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8 \n",
    "\n",
    "\n",
    "print('Convert TFLite model.')\n",
    "QAT_INT8_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24656"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存量化的 tflite 模型\n",
    "open(QAT_INT8_model_path, \"wb\").write(QAT_INT8_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估量化感知训练TF Lite模型的准确率\n",
    "# Define a helper function to evaluate the TF Lite model on the test dataset.\n",
    "def evaluate_model(interpreter):\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "    # Run predictions on every image in the \"test\" dataset.\n",
    "    prediction_digits = []\n",
    "    for i, test_image in enumerate(test_images):\n",
    "        if i % 1000 == 0:\n",
    "            print('Evaluated on {n} results so far.'.format(n=i))\n",
    "        # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "        # the model's input data format.\n",
    "        test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "        # Run inference.\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Post-processing: remove batch dimension and find the digit with highest\n",
    "        # probability.\n",
    "        output = interpreter.tensor(output_index)\n",
    "        digit = np.argmax(output()[0])\n",
    "        prediction_digits.append(digit)\n",
    "\n",
    "    print('\\n')\n",
    "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "    prediction_digits = np.array(prediction_digits)\n",
    "    accuracy = (prediction_digits == test_labels).mean()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 0 results so far.\n",
      "Evaluated on 1000 results so far.\n",
      "Evaluated on 2000 results so far.\n",
      "Evaluated on 3000 results so far.\n",
      "Evaluated on 4000 results so far.\n",
      "Evaluated on 5000 results so far.\n",
      "Evaluated on 6000 results so far.\n",
      "Evaluated on 7000 results so far.\n",
      "Evaluated on 8000 results so far.\n",
      "Evaluated on 9000 results so far.\n",
      "\n",
      "\n",
      "INT8量化后的量化感知训练模型准确度： 0.9747\n"
     ]
    }
   ],
   "source": [
    "# You evaluate the quantized model and see that the accuracy from TensorFlow persists to the TFLite backend.\n",
    "interpreter = tf.lite.Interpreter(model_content=QAT_INT8_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "QAT_test_accuracy = evaluate_model(interpreter)\n",
    "print('INT8量化后的量化感知训练模型准确度：', QAT_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练后量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert TFLite model.\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpav2w__kj/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpav2w__kj/assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter._experimental_new_quantizer = True  # pylint: disable=protected-access\n",
    "\n",
    "# to enable post-training quantization with the representative dataset\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to int8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8 \n",
    "\n",
    "\n",
    "print('Convert TFLite model.')\n",
    "PTQ_INT8_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24168"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存量化的 tflite 模型\n",
    "open(PTQ_INT8_model_path, \"wb\").write(PTQ_INT8_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估量化感知训练TF Lite模型的准确率\n",
    "# Define a helper function to evaluate the TF Lite model on the test dataset.\n",
    "def evaluate_model(interpreter):\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "    # Run predictions on every image in the \"test\" dataset.\n",
    "    prediction_digits = []\n",
    "    for i, test_image in enumerate(test_images):\n",
    "        if i % 1000 == 0:\n",
    "            print('Evaluated on {n} results so far.'.format(n=i))\n",
    "        # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "        # the model's input data format.\n",
    "        test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "        # Run inference.\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Post-processing: remove batch dimension and find the digit with highest\n",
    "        # probability.\n",
    "        output = interpreter.tensor(output_index)\n",
    "        digit = np.argmax(output()[0])\n",
    "        prediction_digits.append(digit)\n",
    "\n",
    "    print('\\n')\n",
    "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "    prediction_digits = np.array(prediction_digits)\n",
    "    accuracy = (prediction_digits == test_labels).mean()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 0 results so far.\n",
      "Evaluated on 1000 results so far.\n",
      "Evaluated on 2000 results so far.\n",
      "Evaluated on 3000 results so far.\n",
      "Evaluated on 4000 results so far.\n",
      "Evaluated on 5000 results so far.\n",
      "Evaluated on 6000 results so far.\n",
      "Evaluated on 7000 results so far.\n",
      "Evaluated on 8000 results so far.\n",
      "Evaluated on 9000 results so far.\n",
      "\n",
      "\n",
      "INT8量化后的训练后量化模型准确度： 0.9733\n"
     ]
    }
   ],
   "source": [
    "# You evaluate the quantized model and see that the accuracy from TensorFlow persists to the TFLite backend.\n",
    "interpreter = tf.lite.Interpreter(model_content=PTQ_INT8_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "PTQ_test_accuracy = evaluate_model(interpreter)\n",
    "print('INT8量化后的训练后量化模型准确度：', PTQ_test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原模型准确率： 0.9732999801635742\n",
      "INT8训练后量化 模型准确度： 0.9733\n",
      "INT8量化感知训练 模型准确度： 0.9747\n"
     ]
    }
   ],
   "source": [
    "print('原模型准确率：', baseline_model_accuracy)\n",
    "print('INT8训练后量化 模型准确度：', PTQ_test_accuracy)\n",
    "print('INT8量化感知训练 模型准确度：', QAT_test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
